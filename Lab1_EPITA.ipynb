{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Miolith/nlp3-labs/blob/master/Lab1_EPITA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMwkByBnaE49"
      },
      "source": [
        "<p style=\"text-align:center;\"><img src=\"data:image/jpeg;base64,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\" alt=\"EPITA Lab 1\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKtcL0CYn2n_"
      },
      "source": [
        "# Part 1. Keywords Extraction (14 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXcmrzodG1pt"
      },
      "source": [
        "## What is Keyword Extraction?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZWKLXCYG9NJ"
      },
      "source": [
        "Keyword extraction is defined as the task that automatically identifies a set of the terms that best describe the subject of document. This is an important method in information retrieval (IR) systems: keywords simplify and speed up the search. Keyword extraction can be used to reduce the dimensionality of text for further text analysis (text classification or topic modeling). S.Art et al., for example, extracted keywords to measure patent similarity. Using keyword extraction, you can automatically index data, summarize a text, or generate tag clouds with the most representative keywords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O3FI910HETW"
      },
      "source": [
        "## How to extract the keywords?\n",
        "All keyword extraction algorithms include the following steps:\n",
        "\n",
        "* Candidate generation. Detection of possible candidate keywords from the text.\n",
        "* Property calculation. Computation of properties and statistics required for ranking.\n",
        "* Ranking. Computation of a score for each candidate keyword and sorting in descending order of all candidates. The top n candidates are finally selected as the n keywords representing the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wD64EYyAHE5k"
      },
      "outputs": [],
      "source": [
        "# all the imports \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr3WHRR0C9Il"
      },
      "source": [
        "## Goal.\n",
        "\n",
        "In the following, given a paper, we will extract the keywords associated to this paper. Each individual can have their own qualitative assessment of what is \"key\" word. However, we will try as much as possible to objectify the approach and quantify to what extent a keyword is indeed key to the paper in question. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoUQe5Df4Bpr"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xT3cUUST9QX6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! git clone https://github.com/MastafaF/ExtractKeywords.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duJTpCLQ9XJ5",
        "outputId": "789a259d-7d7b-4483-b493-255de79282c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.git', 'README.md', 'data.tar.gz', 'LICENSE']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os \n",
        "\n",
        "os.listdir(\"./ExtractKeywords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfVjwTfE9dyP",
        "outputId": "86bf5106-d06f-4c25-b8fe-a2c637b49d47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/\n",
            "data/papers.csv\n"
          ]
        }
      ],
      "source": [
        "# Extract data file \n",
        "\n",
        "! cd ExtractKeywords && tar -zxvf data.tar.gz data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "CdRVtKw44C9p",
        "outputId": "f12c0bce-e39e-4f75-8292-1cdffcdb5628"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-89994590-ae83-445a-8136-d3e913a2298d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89994590-ae83-445a-8136-d3e913a2298d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-89994590-ae83-445a-8136-d3e913a2298d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-89994590-ae83-445a-8136-d3e913a2298d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# load the dataset\n",
        "df = pd.read_csv('./ExtractKeywords/data/papers.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2IElpuJ76o0"
      },
      "source": [
        "## Preprocessing data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRAj0JXjAWBo",
        "outputId": "f0d953b1-ea97-45f7-c4ea-8fd313e7a8fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# For the Lemmatizer \n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHC9ShM-IX7E"
      },
      "source": [
        "### Question 1.1: Preprocessing data in a meaningful way [code] (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BWpI1OJk78Gc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from string import punctuation\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Update stop words accordingly\n",
        "#my_stop_words = STOPWORDS.union(set(['mystopword1', 'mystopword2']))\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "##Creating a list of custom stopwords\n",
        "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
        "             \"show\", \"result\", \"large\", \n",
        "             \"also\", \"one\", \"two\", \"three\", \n",
        "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\n",
        "\n",
        "stop_words = STOPWORDS.union(set(new_words)).union(set(stopwords.words('english')))\n",
        "\n",
        "def pre_process(text):\n",
        "  # ------------------\n",
        "  # Write your implementation here.\n",
        "\n",
        "  # Lowercase\n",
        "  pre_processed_text = text.lower()\n",
        "\n",
        "  # Remove punctuation\n",
        "  pre_processed_text = pre_processed_text.translate(str.maketrans(punctuation, ' '*len(punctuation)))\n",
        " \n",
        "  # Removing stopwords, numbers and single characters\n",
        "  pre_processed_text = [word for word in pre_processed_text.split() if not word in stop_words and not word.isnumeric() and word.isalpha() and len(word) > 1]\n",
        "\n",
        "  # Lemmatization\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  pre_processed_text = [lemmatizer.lemmatize(word) for word in pre_processed_text]\n",
        "\n",
        "  # Join the list of words into a string\n",
        "  pre_processed_text = \" \".join(pre_processed_text)\n",
        "\n",
        "  return pre_processed_text\n",
        "\n",
        "  # ------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZFsz9JX_9I0",
        "outputId": "4c837596-6e85-40ce-df19-1586ad49e44c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 4s, sys: 223 ms, total: 1min 4s\n",
            "Wall time: 1min 5s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "df['preproc_text'] = df['paper_text'].apply(pre_process)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_3ADgBBsAHhg",
        "outputId": "1404cb8e-10f5-40d2-8db9-a618964d6e37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>preproc_text</th>\n",
              "      <td>self organization associative database application hisashi suzuki suguru arimoto osaka university toyonaka osaka japan abstract efficient method self organizing associative database proposed application robot eyesight system proposed database associate input output half discussion algorithm self organization proposed aspect hardware produce new style neural network half applicability handwritten letter recognition autonomous mobile robot demonstrated introduction let mapping given finite infinite set finite infinite set learning machine observes set pair sampled randomly mean cartesian product computes estimate small estimation error measure usually faster decrease estimation error increase number sample better learning machine expression performance incomplete lack consideration candidate assumed preliminarily good learning machine clarify conception let discus type learning machine let advance understanding self organization associative database parameter type ordinary type learning machine assumes equation relating parameter indefinite structure equivalent define implicitly set candidate subset mapping computes value parameter based observed sample type parameter type learning machine defined approach number sample increase alternative case estimation error remains eternally problem designing learning machine return proper structure sense hand assumed structure demanded compact possible achieve fast learning word number parameter small parameter uniquely determined observed sample demand proper contradicts compact consequently parameter type better compactness assumed structure proper better learning machine elementary conception design learning machine universality ordinary neural network suppose sufficient knowledge given unknown case comparatively easy proper compact structure alternative case difficult possible solution compactness assume almighty structure cover combination orthogonal base infinite dimension structure neural network approximation obtained truncating finitely dimension implementation american institute physic main topic designing neural network establish desirable structure work includes developing practical procedure compute value coefficient observed sample discussion flourishing efficient method proposed recently hardware unit computing coefficient parallel speed sold anza mark iii odyssey neural network exists danger error remaining eternally estimating precisely speaking suppose combination base finite number define structure essentially word suppose located near case estimation error negligible distant estimation error negligible research report following situation appears complex estimation error converges value number sample increase decrease hardly dimension heighten property considerable defect neural network recursi type recursive type founded methodology learning follows initial stage set fa instead notation candidate equal set mapping observing xl yl fa reduced fi xt yl observing second fl reduced xt yl candidate set gradually small observation sample proceeds observing sample write likelihood estimation selected fi contrarily parameter type recursive type guarantee surely approach number sample increase recursive type observes yd rewrite value correlated type architecture composed rule rewriting free memory space architecture form naturally kind database build management system data self organizing way database differs ordinary one following sense record sample observed computes estimation database associative database subject constructing associative database establish rule rewri ting purpose adap measure called dissimilari ty dissimilari ty mean mapping real necessarily defined single formula definable example collection rule written form dissimilarity defines structure locally knowledge imperfect flect heuristic way contrarily neural network possible accelerate speed learning establishing especially easily simple process analogically information like human application paper recursive type show strongly effectiveness denote sequence observed sample xl yd simplest construction associative database observing sample follows algorithm initial stage let set let equal min e furthermore add produce sa version improved economize memory follows algorithm initial stage let composed arbitrary element let ii lex equal si min e furthermore ii xi yi let si si add xi yi si produce si si si xi yi construction ii approach increase computation time grows proportionally size si second subject constructing associative database addressing rule employ economize computation time subsequent chapter construction associative database purpose proposed manages data form binary tree self organization associative database given sequence xl yl algorithm constructing associative database follows algorithm step initialization let root root xl yd variable assigned respective node memorize data furthermore let step increase reset pointer root repeat following arrives terminal node leaf notation nand xt let mean descendant node let step display yin related information yin step establish new descendant node secondly let yin yin yin xt finally step loop step stopped time continued suppose gate element artificial synapsis play role branching prepared obtain new style neural network gate element randomly connected algorithm letter recognition recen tly vertical slitting method recognizing typographic english elastic matching method recognizing hand written discrete english global training fuzzy logic search method recognizing chinese character written square style published self organization associative database realizes recognition handwritten continuous english letter wn nov xk la source document loo windowing number sample nualber sampl e experiment scanner take document letter recognizer us parallelogram window cover maximal letter process sequence letter shifting window recognizer scan word slant direction place window left vicinity black point detected window catch letter succeeding letter recognition head letter performed end position boundary line letter known starting scanning boundary repeating operation recognizer accomplishes recursively task major problem come identifying head letter window considering define following regard window image define accordingly denote black point left area boundary window project window measure euclidean distance fj black point closest let summation black point divided number regard couple reading position boundary define accordingly operator teach recognizer interaction relation window reading boundary algorithm precisely recalled reading incorrect operator teach correct reading console boundary position incorrect teach correct position mouse show partially document experiment show change number node recognition rate defined relative frequency correct answer past trial speciiications window height width slant angular example level tree distributed time recognition rate converged experimentally recognition rate converges case rare case attain distinguishable excessive lluctuation writing consistency relation assured like number node increase endlessly clever stop learning recognition rate attains upper limit improve recognition rate consider spelling word future subject obstacle avoiding movement system camera type autonomous mobile robot reported author belongs category mathematical methodology solve usually problem obstacle avoiding movement cost minimization problem cost criterion established artificially contrarily self organization associative database reproduces faithfully cost criterion operator motion robot learning natural length width height robot weight visual angle camera robot following factor motion turn advance control speed experiment passageway wid th inside building author laboratory exist experimental intention arrange box smoking stand gas cylinder stool handcart passage way random let robot camera recall similar trace route preliminarily recorded purpose define following let camera face downward process low pas filter scanning vertically filtered search point luminance change excessively su bstitu te point white point black obstacle exists robot white area show free area robot regard binary image processed define accordingly let number black point exclusive regard image obtained drawing route image define accordingly robot superimposes current camera route recalled inquires operator instruction operator judge subjectively suggested route appropriate negative answer draw desirable route mouse teach new robot opera tion defines implicitly sequence reflecting cost criterion operator iibube roan stationary uni configuration autonomous mobile robot north rmbi unit robot roan experimental environment wall camera preprocessing fa preprocessing course suggest ion search processing obstacle avoiding movement processing position identification define satisfaction rate relative frequency acceptable suggestion route past trial typical experiment change satisfaction rate showed similar tendency attains time notice rest mean directly percentage collision practice prevent collision adopting supplementary measure time number node level tree distributed proposed method reflects delicately character operator example robot trained operator move slowly space obstacle trained operator brush quickly obstacle fact give hint method printing character machine position identification robot identify position recalling similar landscape position data camera purpose principle suffices regard camera image position data respectively memory capacity finite actual compu ters compress camera image slight loss information compression admittable long precision position identification acceptable area major problem come suitable compression method experimental environment jut passageway interval section adjacent jut door robot identifies roughly surrounding landscape section place us temporarily triangular surveying technique exact measure necessary realize task define following turn camera panorama scanning horizontally center line substitute point luminance excessively change black point white regard binary line image processed define accordingly project black point measure euclidean distance black point closest let summation similarly calculate exchanging role denoting number respectively nand define regard positive integer labeled section cf define accordingly learning mode robot check exactly position counter reset periodically operator robot run arbitrarily passageway area learns relation landscape position data position identification area achieved crossing plural database task automatic excepting periodic reset counter kind learning teacher define identification rate relative frequency correct recall position data past trial typical example converged time time number level level oftree distributed identification failure rejected considering trajectory pro blem arises practical use order improve identification rate compression ratio camera image loosened possibility depends improvement hardware future show example actual motion robot based database obstacle avoiding movement position identification example corresponds case moving time interval frame ii actual motion robot conclusion method self organizing associative database proposed application robot eyesight system machine decomposes global structure unknown set local structure known learns universally input output response framework problem implies wide application area example shown paper defect algorithm self organization tree balanced subclass structure subject imposed widen class probable solution abolish addressing rule depending directly value instead establish rule depending distribution function value investigation reference hopfield tank computing neural circuit model science pp rumelhart et al learning representation propagating error nature pp hull hypothesis generation computational model visual word recognition ieee expert fall pp kurtzberg feature analysis symbol recognition elastic matching ibm re develop pp wang suen tree classifier heuristic search global training ieee trans pattern anal mach intell pami pp brook et al self calibration motion stereo vision mobile robot int symp robotics research pp goto stentz cmu mobile robot navigation ieee int conf robotics automation pp madarasz et al design autonomous vehicle disabled ieee jour robotics automation ra pp triendl kriegman stereo vision navigation building ieee int conf robotics automation pp turk et al video road following autonomous land vehicle ieee int conf robotics automation pp</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Visualizing data \n",
        "HTML(pd.DataFrame(df.loc[0, [\"preproc_text\"]]).to_html())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLS9mbe8B4UY"
      },
      "source": [
        "## 0. Raw counts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WY9c6iwI0Lv"
      },
      "source": [
        "### Question 1.2: Build a top N words based on occurence [code] (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "izLhCAH5B-jy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Idea: \n",
        "\n",
        "0. Split with spacy OR nltk \n",
        "\n",
        "1. Counter \n",
        "\n",
        "2. Surface top 10 \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def get_counter(txt_preproc, N=10): \n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    top_N = Counter(txt_preproc.split()).most_common(N)\n",
        "    return top_N\n",
        "    \n",
        "    # ------------------\n",
        "\n",
        "df[\"Top N\"] = df[\"preproc_text\"].apply(get_counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-J0RHMMFW6b",
        "outputId": "d17de3fd-4571-453d-f046-9391bbe9f5b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('input', 58),\n",
              " ('weak', 42),\n",
              " ('synaptic', 36),\n",
              " ('associative', 35),\n",
              " ('ltp', 30),\n",
              " ('strong', 26),\n",
              " ('phase', 26),\n",
              " ('long', 24),\n",
              " ('stimulus', 23),\n",
              " ('hippocampus', 22)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df.loc[2, \"Top N\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK7YbNCyJaqk"
      },
      "source": [
        "### Question 1.3: What are some of the limits of raw counts? How could we improve the approach through preprocessing? [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wao4NivXGPIM"
      },
      "source": [
        "## 1. TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TRcD4MeHFnt"
      },
      "source": [
        "### Introduction.\n",
        "\n",
        "TF-IDF stands for Text Frequency Inverse Document Frequency. The importance of each word increases proportionally to the number of times a word appears in the document (Text Frequency - TF) but is offset by the frequency of the word in the corpus (Inverse Document Frequency - IDF). Using the tf-idf weighting scheme, the keywords are the words with the higherst TF-IDF score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOZe8obbHm-Q"
      },
      "source": [
        "### CountVectorizer to create a vocabulary and generate word counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_pd54cOkGV_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b979db6e-ddf1-40f3-fdfc-35d0c663a421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 8s, sys: 3.93 s, total: 2min 12s\n",
            "Wall time: 2min 18s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "#create a vocabulary of words, \n",
        "cv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n",
        "                   max_features=10000,  # the size of the vocabulary\n",
        "                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n",
        "                  )\n",
        "\n",
        "\n",
        "word_count_vector=cv.fit_transform(df[\"preproc_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fngZVCVGcG_",
        "outputId": "60635330-786b-4db1-f52c-7f79c90b33c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7241x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 5715696 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "word_count_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxV_evcXHuKp"
      },
      "source": [
        "### TfidfTransformer to Compute Inverse Document Frequency (IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCfwFzHCHzOY",
        "outputId": "8a709204-5e20-4400-bd57-aa277e140da2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 23.8 ms, sys: 0 ns, total: 23.8 ms\n",
            "Wall time: 30.1 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,\n",
        "                                   use_idf=True)\n",
        "\n",
        "tfidf_transformer.fit(word_count_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd9hXGWJHzLy",
        "outputId": "d3730a5a-75bc-4873-e47e-f08b9bcbe5de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tfidf_transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1wbWizrJ5CE"
      },
      "source": [
        "### Question 1.4: How can you find an optimal max_df? Why are we using a sparse matrix instead of a regular matrix? [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EekhsGbKYoyv"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2_9miFxKaU1",
        "outputId": "f10d0c67-fb39-4de6-f4aa-da4ec9590d8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 10 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "cv.transform([\" change number node recognition rate defined relative frequency\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "K2Vdn9NTMIxI",
        "outputId": "29deda87-7d74-45aa-c16a-07cdbc3fe1d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x7f367695b3a0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Visualizing data \n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "plt.spy(csr_matrix(cv.transform([\"change number node recognition rate defined relative frequency\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRI_AZtROxP0",
        "outputId": "d27d230c-b224-4a94-a872-f4c3d88b54c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(7280, 0.6385982777409903),\n",
              " (6035, 0.4825314055331922),\n",
              " (3225, 0.28266120934514866),\n",
              " (5896, 0.2670194365214445),\n",
              " (7271, 0.22852928361547203),\n",
              " (7402, 0.2217060832396205),\n",
              " (1166, 0.1919890541213054),\n",
              " (7196, 0.17649246962020493),\n",
              " (2008, 0.15312892047091242),\n",
              " (6013, 0.12378153237392613)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([\"change number node recognition rate defined relative frequency\"]))\n",
        "\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "sorted_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxhq2LiEOxD3",
        "outputId": "093e517f-8cac-4bc2-95dc-9377013fe7cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 10 stored elements in COOrdinate format>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "coo_matrix = tf_idf_vector.tocoo()\n",
        "# list(zip(coo_matrix.col, coo_matrix.data))\n",
        "coo_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "G4EwdqH_Gm0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73248790-8da0-44ba-b4d6-57476230d86a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# get feature names\n",
        "feature_names=cv.get_feature_names()\n",
        "\n",
        "def get_keywords(txt, top_N=10):\n",
        "\n",
        "  # ------------------\n",
        "  # Write your implementation here.\n",
        "\n",
        "  tf_idf_vector=tfidf_transformer.transform(cv.transform([txt]))\n",
        "  sorted_keyword_score=sort_coo(tf_idf_vector.tocoo())\n",
        "  sorted_keyword_score = sorted_keyword_score[:top_N]\n",
        "  sorted_keyword_score = [(feature_names[i], round(score, 3)) for i, score in sorted_keyword_score]\n",
        "\n",
        "  # ------------------\n",
        "  \n",
        "  return sorted_keyword_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahvTo9gjSWCW",
        "outputId": "4b6f3fe6-2aa8-402d-be35-1e74c8b1b16a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('recognition rate', 0.639),\n",
              " ('number node', 0.483),\n",
              " ('frequency', 0.283),\n",
              " ('node', 0.267),\n",
              " ('recognition', 0.229),\n",
              " ('relative', 0.222),\n",
              " ('change', 0.192),\n",
              " ('rate', 0.176),\n",
              " ('defined', 0.153),\n",
              " ('number', 0.124)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "get_keywords(txt=\"change number node recognition rate defined relative frequency\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9hzsg0CUQZ_"
      },
      "source": [
        "### Compare Raw Counts to Tf-IDF approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ot6GLVIiUPqj"
      },
      "outputs": [],
      "source": [
        "df[\"Top_N_TF-IDF\"] = df[\"preproc_text\"].apply(get_keywords, top_N=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "g4sydV20UjvA",
        "outputId": "9b9f6af8-1811-41fc-a74e-19166f26490e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  year                                              title  \\\n",
              "1477  2343  2002  Dynamic Bayesian Networks with Deterministic L...   \n",
              "\n",
              "     event_type                                           pdf_name  \\\n",
              "1477        NaN  2343-dynamic-bayesian-networks-with-determinis...   \n",
              "\n",
              "              abstract                                         paper_text  \\\n",
              "1477  Abstract Missing  Dynamic Bayesian Networks with\\nDeterministic ...   \n",
              "\n",
              "                                           preproc_text  \\\n",
              "1477  dynamic bayesian network deterministic latent ...   \n",
              "\n",
              "                                                  Top N  \\\n",
              "1477  [(hidden, 69), (model, 59), (unit, 35), (netwo...   \n",
              "\n",
              "                                           Top_N_TF-IDF  \n",
              "1477  [(hidden, 0.472), (hidden unit, 0.293), (visib...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88b6a339-2084-4d42-bc53-5680181ae120\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "      <th>preproc_text</th>\n",
              "      <th>Top N</th>\n",
              "      <th>Top_N_TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1477</th>\n",
              "      <td>2343</td>\n",
              "      <td>2002</td>\n",
              "      <td>Dynamic Bayesian Networks with Deterministic L...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2343-dynamic-bayesian-networks-with-determinis...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Dynamic Bayesian Networks with\\nDeterministic ...</td>\n",
              "      <td>dynamic bayesian network deterministic latent ...</td>\n",
              "      <td>[(hidden, 69), (model, 59), (unit, 35), (netwo...</td>\n",
              "      <td>[(hidden, 0.472), (hidden unit, 0.293), (visib...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88b6a339-2084-4d42-bc53-5680181ae120')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88b6a339-2084-4d42-bc53-5680181ae120 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88b6a339-2084-4d42-bc53-5680181ae120');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "df.sample(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkDLV8yILNNS"
      },
      "source": [
        "### Question 1.5: Find an example where there is a noticeable difference between tf-idf and raw counts? Justify which method you would choose yourself (there is no bad and good answer here) [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mes_RnLNVXBX"
      },
      "source": [
        "## 2. KeyBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7iWThDYXlJY"
      },
      "source": [
        "## 2.0. Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "g0Ji149nXmUU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "pip install keybert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "k-kXWUGKXqIB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from keybert import KeyBERT\n",
        "\n",
        "doc = \"\"\"\n",
        "         Supervised learning is the machine learning task of learning a function that\n",
        "         maps an input to an output based on example input-output pairs. It infers a\n",
        "         function from labeled training data consisting of a set of training examples.\n",
        "         In supervised learning, each example is a pair consisting of an input object\n",
        "         (typically a vector) and a desired output value (also called the supervisory signal). \n",
        "         A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
        "         which can be used for mapping new examples. An optimal scenario will allow for the \n",
        "         algorithm to correctly determine the class labels for unseen instances. This requires \n",
        "         the learning algorithm to generalize from the training data to unseen situations in a \n",
        "         'reasonable' way (see inductive bias).\n",
        "      \"\"\"\n",
        "kw_model = KeyBERT()\n",
        "keywords = kw_model.extract_keywords(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNrYae2jYDqU",
        "outputId": "25403d82-9f47-4937-fad8-0435e64aaaea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('supervised', 0.6676),\n",
              " ('labeled', 0.4896),\n",
              " ('learning', 0.4813),\n",
              " ('training', 0.4134),\n",
              " ('labels', 0.3947)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJs_f30AYmKN"
      },
      "source": [
        "### Question 2.0. Apply KeyBERT to the a sample of the dataset [code] (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7GisGAL0pIDy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "be173b15-46a3-4e93-b4b5-b9760ebd9d2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  year                                             title event_type  \\\n",
              "3301  3990  2010  Multitask Learning without Label Correspondences        NaN   \n",
              "\n",
              "                                               pdf_name  \\\n",
              "3301  3990-multitask-learning-without-label-correspo...   \n",
              "\n",
              "                                               abstract  \\\n",
              "3301  We propose an algorithm to perform multitask l...   \n",
              "\n",
              "                                             paper_text  \\\n",
              "3301  Multitask Learning without Label Correspondenc...   \n",
              "\n",
              "                                           preproc_text  \\\n",
              "3301  multitask learning label correspondence novi a...   \n",
              "\n",
              "                                                  Top N  \\\n",
              "3301  [(task, 69), (label, 52), (learning, 45), (set...   \n",
              "\n",
              "                                           Top_N_TF-IDF  \n",
              "3301  [(yahoo, 0.33), (label, 0.287), (task, 0.265),...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df9bdb8d-2cba-41c6-8ac0-184ce61df097\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "      <th>preproc_text</th>\n",
              "      <th>Top N</th>\n",
              "      <th>Top_N_TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3301</th>\n",
              "      <td>3990</td>\n",
              "      <td>2010</td>\n",
              "      <td>Multitask Learning without Label Correspondences</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3990-multitask-learning-without-label-correspo...</td>\n",
              "      <td>We propose an algorithm to perform multitask l...</td>\n",
              "      <td>Multitask Learning without Label Correspondenc...</td>\n",
              "      <td>multitask learning label correspondence novi a...</td>\n",
              "      <td>[(task, 69), (label, 52), (learning, 45), (set...</td>\n",
              "      <td>[(yahoo, 0.33), (label, 0.287), (task, 0.265),...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df9bdb8d-2cba-41c6-8ac0-184ce61df097')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-df9bdb8d-2cba-41c6-8ac0-184ce61df097 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-df9bdb8d-2cba-41c6-8ac0-184ce61df097');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "df_ = df.sample(100)\n",
        "df_.sample(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN3Dlu11YwLE",
        "outputId": "f17b817c-ca1f-4a8a-d063-515fd31880fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 42s, sys: 14.8 s, total: 2min 56s\n",
            "Wall time: 2min 49s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%%capture\n",
        "\n",
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "df_[\"Top_N_KeyBERT\"] = df_[\"preproc_text\"].apply(lambda x: kw_model.extract_keywords(x, keyphrase_ngram_range=(1, 3), stop_words=stop_words, use_maxsum=True, nr_candidates=10, top_n=10))\n",
        "\n",
        "\n",
        "# ------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "wYDNwWa5ZiTK",
        "outputId": "dcf1de1d-befe-4a45-d877-88a1863789a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Top N Counter: '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('metric', 121),\n",
              " ('geodesic', 59),\n",
              " ('space', 50),\n",
              " ('tensor', 46),\n",
              " ('data', 37),\n",
              " ('distance', 35),\n",
              " ('manifold', 34),\n",
              " ('point', 33),\n",
              " ('learning', 29),\n",
              " ('riemannian', 29)]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Top N TF-IDF: '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('geodesic', 0.492),\n",
              " ('metric', 0.487),\n",
              " ('tensor', 0.288),\n",
              " ('riemannian', 0.231),\n",
              " ('manifold', 0.189),\n",
              " ('tangent', 0.155),\n",
              " ('tangent space', 0.146),\n",
              " ('euclidean', 0.117),\n",
              " ('feature space', 0.116),\n",
              " ('distance', 0.108)]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Top N KeyBERT: '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('learning metric specifically', 0.6169),\n",
              " ('metric learning euclidean', 0.6245),\n",
              " ('metric learning corresponds', 0.6295),\n",
              " ('multi metric learning', 0.6336),\n",
              " ('learn metric', 0.6351),\n",
              " ('learning multiple metric', 0.6479),\n",
              " ('learn separate metric', 0.6498),\n",
              " ('metric learning based', 0.6506),\n",
              " ('geometric metric learning', 0.667),\n",
              " ('metric learning technique', 0.6859)]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# TODO: compare the same paper example across the 3 methods \n",
        "\n",
        "idx_focus = 3909\n",
        "\n",
        "display(\"Top N Counter: \", df_.loc[idx_focus, \"Top N\"])\n",
        "print()\n",
        "display(\"Top N TF-IDF: \", df_.loc[idx_focus, \"Top_N_TF-IDF\"])\n",
        "print()\n",
        "display(\"Top N KeyBERT: \", df_.loc[idx_focus, \"Top_N_KeyBERT\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9GQQGMTZgrR"
      },
      "source": [
        "### Question 2.2. Comparison of multilple techniques [written] (4 points)\n",
        "\n",
        "1. Draw a table of the solution, the quality score that you defined and the time taken to find keywords across a sample of 1000 of the original dataset. \n",
        "2. Can you think of tweaks to reduce time to compute? If yes, add an additional column to the above table with your proposed tweaks.\n",
        "3. Based on the above table and  lecture 1, what do you think is the most appropriate solution for keywords extraction? Why? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhS_Y6qgqW4Z"
      },
      "source": [
        "# Part 2. Word Vectors (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kedMB9f-qsnw",
        "outputId": "b405eaea-cfeb-487b-9583-5d1fda435f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 5]\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "from nltk.corpus import reuters\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy as sp\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5S4yJ_ZrHAo"
      },
      "source": [
        "Word Vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc., so it is important to build some intuitions as to their strengths and weaknesses. Here, you will explore two types of word vectors: those derived from co-occurrence matrices, and those derived via GloVe.\n",
        "\n",
        "Note on Terminology: The terms \"word vectors\" and \"word embeddings\" are often used interchangeably. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space. As Wikipedia states, \"conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKcD1SUIrP_m"
      },
      "source": [
        "## Count-Based Word Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uvm6lbSsFD0"
      },
      "source": [
        "Most word vector models start from the following idea:\n",
        "\n",
        "You shall know a word by the company it keeps (Firth, J. R. 1957:11)\n",
        "\n",
        "Many word vector implementations are driven by the idea that similar words, i.e., (near) synonyms, will be used in similar contexts. As a result, similar words will often be spoken or written along with a shared subset of words, i.e., contexts. By examining these contexts, we can try to develop embeddings for our words. With this intuition in mind, many \"old school\" approaches to constructing word vectors relied on word counts. Here we elaborate upon one of those strategies, co-occurrence matrices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vMxbozcslLA"
      },
      "source": [
        "## Plotting Co-Occurrence Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3OO_oowsrK2"
      },
      "source": [
        "\n",
        "Here, we will be using the Reuters (business and financial news) corpus. If you haven't run the import cell at the top of this page, please run it now (click it and press SHIFT-RETURN). The corpus consists of 10,788 news documents totaling 1.3 million words. These documents span 90 categories and are split into train and test. For more details, please see https://www.nltk.org/book/ch02.html. We provide a read_corpus function below that pulls out only articles from the \"crude\" (i.e. news articles about oil, gas, etc.) category. The function also adds <START> and <END> tokens to each of the documents, and lowercases words. You do not have to perform any other kind of pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "0xTQwympsqDq"
      },
      "outputs": [],
      "source": [
        "def read_corpus(category=\"crude\"):\n",
        "    \"\"\" Read files from the specified Reuter's category.\n",
        "        Params:\n",
        "            category (string): category name\n",
        "        Return:\n",
        "            list of lists, with words from each of the processed files\n",
        "    \"\"\"\n",
        "    files = reuters.fileids(category)\n",
        "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQrwL93ns1Qy"
      },
      "source": [
        "Let's have a look what these documents are like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "0eZvFI3Qs0x4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "206a6fd5-1d34-4be0-ae98-85d94b19ac65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['<START>', 'japan', 'to', 'revise', 'long', '-', 'term', 'energy', 'demand', 'downwards', 'the',\n",
            "  'ministry', 'of', 'international', 'trade', 'and', 'industry', '(', 'miti', ')', 'will', 'revise',\n",
            "  'its', 'long', '-', 'term', 'energy', 'supply', '/', 'demand', 'outlook', 'by', 'august', 'to',\n",
            "  'meet', 'a', 'forecast', 'downtrend', 'in', 'japanese', 'energy', 'demand', ',', 'ministry',\n",
            "  'officials', 'said', '.', 'miti', 'is', 'expected', 'to', 'lower', 'the', 'projection', 'for',\n",
            "  'primary', 'energy', 'supplies', 'in', 'the', 'year', '2000', 'to', '550', 'mln', 'kilolitres',\n",
            "  '(', 'kl', ')', 'from', '600', 'mln', ',', 'they', 'said', '.', 'the', 'decision', 'follows',\n",
            "  'the', 'emergence', 'of', 'structural', 'changes', 'in', 'japanese', 'industry', 'following',\n",
            "  'the', 'rise', 'in', 'the', 'value', 'of', 'the', 'yen', 'and', 'a', 'decline', 'in', 'domestic',\n",
            "  'electric', 'power', 'demand', '.', 'miti', 'is', 'planning', 'to', 'work', 'out', 'a', 'revised',\n",
            "  'energy', 'supply', '/', 'demand', 'outlook', 'through', 'deliberations', 'of', 'committee',\n",
            "  'meetings', 'of', 'the', 'agency', 'of', 'natural', 'resources', 'and', 'energy', ',', 'the',\n",
            "  'officials', 'said', '.', 'they', 'said', 'miti', 'will', 'also', 'review', 'the', 'breakdown',\n",
            "  'of', 'energy', 'supply', 'sources', ',', 'including', 'oil', ',', 'nuclear', ',', 'coal', 'and',\n",
            "  'natural', 'gas', '.', 'nuclear', 'energy', 'provided', 'the', 'bulk', 'of', 'japan', \"'\", 's',\n",
            "  'electric', 'power', 'in', 'the', 'fiscal', 'year', 'ended', 'march', '31', ',', 'supplying',\n",
            "  'an', 'estimated', '27', 'pct', 'on', 'a', 'kilowatt', '/', 'hour', 'basis', ',', 'followed',\n",
            "  'by', 'oil', '(', '23', 'pct', ')', 'and', 'liquefied', 'natural', 'gas', '(', '21', 'pct', '),',\n",
            "  'they', 'noted', '.', '<END>'],\n",
            " ['<START>', 'energy', '/', 'u', '.', 's', '.', 'petrochemical', 'industry', 'cheap', 'oil',\n",
            "  'feedstocks', ',', 'the', 'weakened', 'u', '.', 's', '.', 'dollar', 'and', 'a', 'plant',\n",
            "  'utilization', 'rate', 'approaching', '90', 'pct', 'will', 'propel', 'the', 'streamlined', 'u',\n",
            "  '.', 's', '.', 'petrochemical', 'industry', 'to', 'record', 'profits', 'this', 'year', ',',\n",
            "  'with', 'growth', 'expected', 'through', 'at', 'least', '1990', ',', 'major', 'company',\n",
            "  'executives', 'predicted', '.', 'this', 'bullish', 'outlook', 'for', 'chemical', 'manufacturing',\n",
            "  'and', 'an', 'industrywide', 'move', 'to', 'shed', 'unrelated', 'businesses', 'has', 'prompted',\n",
            "  'gaf', 'corp', '&', 'lt', ';', 'gaf', '>,', 'privately', '-', 'held', 'cain', 'chemical', 'inc',\n",
            "  ',', 'and', 'other', 'firms', 'to', 'aggressively', 'seek', 'acquisitions', 'of', 'petrochemical',\n",
            "  'plants', '.', 'oil', 'companies', 'such', 'as', 'ashland', 'oil', 'inc', '&', 'lt', ';', 'ash',\n",
            "  '>,', 'the', 'kentucky', '-', 'based', 'oil', 'refiner', 'and', 'marketer', ',', 'are', 'also',\n",
            "  'shopping', 'for', 'money', '-', 'making', 'petrochemical', 'businesses', 'to', 'buy', '.', '\"',\n",
            "  'i', 'see', 'us', 'poised', 'at', 'the', 'threshold', 'of', 'a', 'golden', 'period', ',\"', 'said',\n",
            "  'paul', 'oreffice', ',', 'chairman', 'of', 'giant', 'dow', 'chemical', 'co', '&', 'lt', ';',\n",
            "  'dow', '>,', 'adding', ',', '\"', 'there', \"'\", 's', 'no', 'major', 'plant', 'capacity', 'being',\n",
            "  'added', 'around', 'the', 'world', 'now', '.', 'the', 'whole', 'game', 'is', 'bringing', 'out',\n",
            "  'new', 'products', 'and', 'improving', 'the', 'old', 'ones', '.\"', 'analysts', 'say', 'the',\n",
            "  'chemical', 'industry', \"'\", 's', 'biggest', 'customers', ',', 'automobile', 'manufacturers',\n",
            "  'and', 'home', 'builders', 'that', 'use', 'a', 'lot', 'of', 'paints', 'and', 'plastics', ',',\n",
            "  'are', 'expected', 'to', 'buy', 'quantities', 'this', 'year', '.', 'u', '.', 's', '.',\n",
            "  'petrochemical', 'plants', 'are', 'currently', 'operating', 'at', 'about', '90', 'pct',\n",
            "  'capacity', ',', 'reflecting', 'tighter', 'supply', 'that', 'could', 'hike', 'product', 'prices',\n",
            "  'by', '30', 'to', '40', 'pct', 'this', 'year', ',', 'said', 'john', 'dosher', ',', 'managing',\n",
            "  'director', 'of', 'pace', 'consultants', 'inc', 'of', 'houston', '.', 'demand', 'for', 'some',\n",
            "  'products', 'such', 'as', 'styrene', 'could', 'push', 'profit', 'margins', 'up', 'by', 'as',\n",
            "  'much', 'as', '300', 'pct', ',', 'he', 'said', '.', 'oreffice', ',', 'speaking', 'at', 'a',\n",
            "  'meeting', 'of', 'chemical', 'engineers', 'in', 'houston', ',', 'said', 'dow', 'would', 'easily',\n",
            "  'top', 'the', '741', 'mln', 'dlrs', 'it', 'earned', 'last', 'year', 'and', 'predicted', 'it',\n",
            "  'would', 'have', 'the', 'best', 'year', 'in', 'its', 'history', '.', 'in', '1985', ',', 'when',\n",
            "  'oil', 'prices', 'were', 'still', 'above', '25', 'dlrs', 'a', 'barrel', 'and', 'chemical',\n",
            "  'exports', 'were', 'adversely', 'affected', 'by', 'the', 'strong', 'u', '.', 's', '.', 'dollar',\n",
            "  ',', 'dow', 'had', 'profits', 'of', '58', 'mln', 'dlrs', '.', '\"', 'i', 'believe', 'the',\n",
            "  'entire', 'chemical', 'industry', 'is', 'headed', 'for', 'a', 'record', 'year', 'or', 'close',\n",
            "  'to', 'it', ',\"', 'oreffice', 'said', '.', 'gaf', 'chairman', 'samuel', 'heyman', 'estimated',\n",
            "  'that', 'the', 'u', '.', 's', '.', 'chemical', 'industry', 'would', 'report', 'a', '20', 'pct',\n",
            "  'gain', 'in', 'profits', 'during', '1987', '.', 'last', 'year', ',', 'the', 'domestic',\n",
            "  'industry', 'earned', 'a', 'total', 'of', '13', 'billion', 'dlrs', ',', 'a', '54', 'pct', 'leap',\n",
            "  'from', '1985', '.', 'the', 'turn', 'in', 'the', 'fortunes', 'of', 'the', 'once', '-', 'sickly',\n",
            "  'chemical', 'industry', 'has', 'been', 'brought', 'about', 'by', 'a', 'combination', 'of', 'luck',\n",
            "  'and', 'planning', ',', 'said', 'pace', \"'\", 's', 'john', 'dosher', '.', 'dosher', 'said', 'last',\n",
            "  'year', \"'\", 's', 'fall', 'in', 'oil', 'prices', 'made', 'feedstocks', 'dramatically', 'cheaper',\n",
            "  'and', 'at', 'the', 'same', 'time', 'the', 'american', 'dollar', 'was', 'weakening', 'against',\n",
            "  'foreign', 'currencies', '.', 'that', 'helped', 'boost', 'u', '.', 's', '.', 'chemical',\n",
            "  'exports', '.', 'also', 'helping', 'to', 'bring', 'supply', 'and', 'demand', 'into', 'balance',\n",
            "  'has', 'been', 'the', 'gradual', 'market', 'absorption', 'of', 'the', 'extra', 'chemical',\n",
            "  'manufacturing', 'capacity', 'created', 'by', 'middle', 'eastern', 'oil', 'producers', 'in',\n",
            "  'the', 'early', '1980s', '.', 'finally', ',', 'virtually', 'all', 'major', 'u', '.', 's', '.',\n",
            "  'chemical', 'manufacturers', 'have', 'embarked', 'on', 'an', 'extensive', 'corporate',\n",
            "  'restructuring', 'program', 'to', 'mothball', 'inefficient', 'plants', ',', 'trim', 'the',\n",
            "  'payroll', 'and', 'eliminate', 'unrelated', 'businesses', '.', 'the', 'restructuring', 'touched',\n",
            "  'off', 'a', 'flurry', 'of', 'friendly', 'and', 'hostile', 'takeover', 'attempts', '.', 'gaf', ',',\n",
            "  'which', 'made', 'an', 'unsuccessful', 'attempt', 'in', '1985', 'to', 'acquire', 'union',\n",
            "  'carbide', 'corp', '&', 'lt', ';', 'uk', '>,', 'recently', 'offered', 'three', 'billion', 'dlrs',\n",
            "  'for', 'borg', 'warner', 'corp', '&', 'lt', ';', 'bor', '>,', 'a', 'chicago', 'manufacturer',\n",
            "  'of', 'plastics', 'and', 'chemicals', '.', 'another', 'industry', 'powerhouse', ',', 'w', '.',\n",
            "  'r', '.', 'grace', '&', 'lt', ';', 'gra', '>', 'has', 'divested', 'its', 'retailing', ',',\n",
            "  'restaurant', 'and', 'fertilizer', 'businesses', 'to', 'raise', 'cash', 'for', 'chemical',\n",
            "  'acquisitions', '.', 'but', 'some', 'experts', 'worry', 'that', 'the', 'chemical', 'industry',\n",
            "  'may', 'be', 'headed', 'for', 'trouble', 'if', 'companies', 'continue', 'turning', 'their',\n",
            "  'back', 'on', 'the', 'manufacturing', 'of', 'staple', 'petrochemical', 'commodities', ',', 'such',\n",
            "  'as', 'ethylene', ',', 'in', 'favor', 'of', 'more', 'profitable', 'specialty', 'chemicals',\n",
            "  'that', 'are', 'custom', '-', 'designed', 'for', 'a', 'small', 'group', 'of', 'buyers', '.', '\"',\n",
            "  'companies', 'like', 'dupont', '&', 'lt', ';', 'dd', '>', 'and', 'monsanto', 'co', '&', 'lt', ';',\n",
            "  'mtc', '>', 'spent', 'the', 'past', 'two', 'or', 'three', 'years', 'trying', 'to', 'get', 'out',\n",
            "  'of', 'the', 'commodity', 'chemical', 'business', 'in', 'reaction', 'to', 'how', 'badly', 'the',\n",
            "  'market', 'had', 'deteriorated', ',\"', 'dosher', 'said', '.', '\"', 'but', 'i', 'think', 'they',\n",
            "  'will', 'eventually', 'kill', 'the', 'margins', 'on', 'the', 'profitable', 'chemicals', 'in',\n",
            "  'the', 'niche', 'market', '.\"', 'some', 'top', 'chemical', 'executives', 'share', 'the',\n",
            "  'concern', '.', '\"', 'the', 'challenge', 'for', 'our', 'industry', 'is', 'to', 'keep', 'from',\n",
            "  'getting', 'carried', 'away', 'and', 'repeating', 'past', 'mistakes', ',\"', 'gaf', \"'\", 's',\n",
            "  'heyman', 'cautioned', '.', '\"', 'the', 'shift', 'from', 'commodity', 'chemicals', 'may', 'be',\n",
            "  'ill', '-', 'advised', '.', 'specialty', 'businesses', 'do', 'not', 'stay', 'special', 'long',\n",
            "  '.\"', 'houston', '-', 'based', 'cain', 'chemical', ',', 'created', 'this', 'month', 'by', 'the',\n",
            "  'sterling', 'investment', 'banking', 'group', ',', 'believes', 'it', 'can', 'generate', '700',\n",
            "  'mln', 'dlrs', 'in', 'annual', 'sales', 'by', 'bucking', 'the', 'industry', 'trend', '.',\n",
            "  'chairman', 'gordon', 'cain', ',', 'who', 'previously', 'led', 'a', 'leveraged', 'buyout', 'of',\n",
            "  'dupont', \"'\", 's', 'conoco', 'inc', \"'\", 's', 'chemical', 'business', ',', 'has', 'spent', '1',\n",
            "  '.', '1', 'billion', 'dlrs', 'since', 'january', 'to', 'buy', 'seven', 'petrochemical', 'plants',\n",
            "  'along', 'the', 'texas', 'gulf', 'coast', '.', 'the', 'plants', 'produce', 'only', 'basic',\n",
            "  'commodity', 'petrochemicals', 'that', 'are', 'the', 'building', 'blocks', 'of', 'specialty',\n",
            "  'products', '.', '\"', 'this', 'kind', 'of', 'commodity', 'chemical', 'business', 'will', 'never',\n",
            "  'be', 'a', 'glamorous', ',', 'high', '-', 'margin', 'business', ',\"', 'cain', 'said', ',',\n",
            "  'adding', 'that', 'demand', 'is', 'expected', 'to', 'grow', 'by', 'about', 'three', 'pct',\n",
            "  'annually', '.', 'garo', 'armen', ',', 'an', 'analyst', 'with', 'dean', 'witter', 'reynolds', ',',\n",
            "  'said', 'chemical', 'makers', 'have', 'also', 'benefitted', 'by', 'increasing', 'demand', 'for',\n",
            "  'plastics', 'as', 'prices', 'become', 'more', 'competitive', 'with', 'aluminum', ',', 'wood',\n",
            "  'and', 'steel', 'products', '.', 'armen', 'estimated', 'the', 'upturn', 'in', 'the', 'chemical',\n",
            "  'business', 'could', 'last', 'as', 'long', 'as', 'four', 'or', 'five', 'years', ',', 'provided',\n",
            "  'the', 'u', '.', 's', '.', 'economy', 'continues', 'its', 'modest', 'rate', 'of', 'growth', '.',\n",
            "  '<END>'],\n",
            " ['<START>', 'turkey', 'calls', 'for', 'dialogue', 'to', 'solve', 'dispute', 'turkey', 'said',\n",
            "  'today', 'its', 'disputes', 'with', 'greece', ',', 'including', 'rights', 'on', 'the',\n",
            "  'continental', 'shelf', 'in', 'the', 'aegean', 'sea', ',', 'should', 'be', 'solved', 'through',\n",
            "  'negotiations', '.', 'a', 'foreign', 'ministry', 'statement', 'said', 'the', 'latest', 'crisis',\n",
            "  'between', 'the', 'two', 'nato', 'members', 'stemmed', 'from', 'the', 'continental', 'shelf',\n",
            "  'dispute', 'and', 'an', 'agreement', 'on', 'this', 'issue', 'would', 'effect', 'the', 'security',\n",
            "  ',', 'economy', 'and', 'other', 'rights', 'of', 'both', 'countries', '.', '\"', 'as', 'the',\n",
            "  'issue', 'is', 'basicly', 'political', ',', 'a', 'solution', 'can', 'only', 'be', 'found', 'by',\n",
            "  'bilateral', 'negotiations', ',\"', 'the', 'statement', 'said', '.', 'greece', 'has', 'repeatedly',\n",
            "  'said', 'the', 'issue', 'was', 'legal', 'and', 'could', 'be', 'solved', 'at', 'the',\n",
            "  'international', 'court', 'of', 'justice', '.', 'the', 'two', 'countries', 'approached', 'armed',\n",
            "  'confrontation', 'last', 'month', 'after', 'greece', 'announced', 'it', 'planned', 'oil',\n",
            "  'exploration', 'work', 'in', 'the', 'aegean', 'and', 'turkey', 'said', 'it', 'would', 'also',\n",
            "  'search', 'for', 'oil', '.', 'a', 'face', '-', 'off', 'was', 'averted', 'when', 'turkey',\n",
            "  'confined', 'its', 'research', 'to', 'territorrial', 'waters', '.', '\"', 'the', 'latest',\n",
            "  'crises', 'created', 'an', 'historic', 'opportunity', 'to', 'solve', 'the', 'disputes', 'between',\n",
            "  'the', 'two', 'countries', ',\"', 'the', 'foreign', 'ministry', 'statement', 'said', '.', 'turkey',\n",
            "  \"'\", 's', 'ambassador', 'in', 'athens', ',', 'nazmi', 'akiman', ',', 'was', 'due', 'to', 'meet',\n",
            "  'prime', 'minister', 'andreas', 'papandreou', 'today', 'for', 'the', 'greek', 'reply', 'to', 'a',\n",
            "  'message', 'sent', 'last', 'week', 'by', 'turkish', 'prime', 'minister', 'turgut', 'ozal', '.',\n",
            "  'the', 'contents', 'of', 'the', 'message', 'were', 'not', 'disclosed', '.', '<END>']]\n"
          ]
        }
      ],
      "source": [
        "reuters_corpus = read_corpus()\n",
        "pprint.pprint(reuters_corpus[:3], compact=True, width=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNKy6j3as7xJ"
      },
      "source": [
        "### Question 2.1: Implement distinct_words [code] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIgkQ47otdqZ"
      },
      "source": [
        "Write a method to work out the distinct words (word types) that occur in the corpus. You can do this with for loops, but it's more efficient to do it with Python list comprehensions. In particular, this may be useful to flatten a list of lists. If you're not familiar with Python list comprehensions in general, here's more information.\n",
        "\n",
        "Your returned corpus_words should be sorted. You can use python's sorted function for this.\n",
        "\n",
        "You may find it useful to use Python sets to remove duplicate words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "VTIH5vFetgjD"
      },
      "outputs": [],
      "source": [
        "def distinct_words(corpus):\n",
        "    \"\"\" Determine a list of distinct words for the corpus.\n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents - eg [[\"hey\", \"I\", \"am\", \"toto\"], [\"hey\", \"I\", \"am\", \"tata\"]]\n",
        "        Return:\n",
        "            corpus_words (list of strings): sorted list of distinct words across the corpus\n",
        "            num_corpus_words (integer): number of distinct words across the corpus\n",
        "    \"\"\"\n",
        "    \n",
        "    # ------------------\n",
        "    # Write your implementation here.w\n",
        "    corpus_words = sorted(list(set([word for doc in corpus for word in doc])))\n",
        "    num_corpus_words = len(corpus_words)\n",
        "    # ------------------\n",
        "\n",
        "    return corpus_words, num_corpus_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZX4dH8stmYN",
        "outputId": "68dcd3ab-fbd7-4109-aad5-1c8d9d02dc38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "test_corpus_words, num_corpus_words = distinct_words(test_corpus)\n",
        "\n",
        "# Correct answers\n",
        "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
        "ans_num_corpus_words = len(ans_test_corpus_words)\n",
        "\n",
        "# Test correct number of words\n",
        "assert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\n",
        "\n",
        "# Test correct words\n",
        "assert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86fD2hYr3fw8"
      },
      "source": [
        "### Question 2.2: Implement compute_co_occurrence_matrix [code] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE4MLCIa3lKw"
      },
      "source": [
        "Write a method that constructs a co-occurrence matrix for a certain window-size  n  (with a default of 4), considering words  n  before and  n  after the word in the center of the window. Here, we start to use numpy (np) to represent vectors, matrices, and tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "zz5vrGb43lbA"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from collections import Counter \n",
        "\n",
        "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
        "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
        "    \n",
        "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
        "              number of co-occurring words.\n",
        "              \n",
        "              For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\n",
        "              \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\n",
        "    \n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "            window_size (int): size of context window\n",
        "        Return:\n",
        "            M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): \n",
        "                Co-occurence matrix of word counts. \n",
        "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
        "            word2ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
        "    \"\"\"\n",
        "    words, num_words = distinct_words(corpus)\n",
        "    \n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    \n",
        "    word2ind = {word: i for i, word in enumerate(words)}\n",
        "    M = np.zeros((num_words, num_words))\n",
        "    for doc in corpus:\n",
        "        for i, word in enumerate(doc):\n",
        "            for j in range(max(0, i - window_size), min(len(doc), i + window_size + 1)):\n",
        "                if i != j:\n",
        "                    M[word2ind[word], word2ind[doc[j]]] = 1\n",
        "                    M[word2ind[doc[j]], word2ind[word]] = 1\n",
        "    # ------------------\n",
        "\n",
        "    return M, word2ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guUdCsM2BUuC",
        "outputId": "d6049761-db23-4f59-8a4c-4798c51702bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and get student's co-occurrence matrix\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "M_test, word2ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "\n",
        "# Correct M and word2ind\n",
        "M_test_ans = np.array( \n",
        "    [[0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,],\n",
        "     [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,],\n",
        "     [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,],\n",
        "     [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,],\n",
        "     [0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,],\n",
        "     [1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,]]\n",
        ")\n",
        "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
        "word2ind_ans = dict(zip(ans_test_corpus_words, range(len(ans_test_corpus_words))))\n",
        "\n",
        "# Test correct word2ind\n",
        "assert (word2ind_ans == word2ind_test), \"Your word2ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2ind_ans, word2ind_test)\n",
        "\n",
        "# Test correct M shape\n",
        "assert (M_test.shape == M_test_ans.shape), \"M matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(M_test.shape, M_test_ans.shape)\n",
        "\n",
        "# Test correct M values\n",
        "for w1 in word2ind_ans.keys():\n",
        "    idx1 = word2ind_ans[w1]\n",
        "    for w2 in word2ind_ans.keys():\n",
        "        idx2 = word2ind_ans[w2]\n",
        "        student = M_test[idx1, idx2]\n",
        "        correct = M_test_ans[idx1, idx2]\n",
        "        if student != correct:\n",
        "            print(\"Correct M:\")\n",
        "            print(M_test_ans)\n",
        "            print(\"Your M: \")\n",
        "            print(M_test)\n",
        "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix M. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student, correct))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCC24T0WPyI2"
      },
      "source": [
        "### Question 2.3: Implement reduce_to_k_dim [code] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ9XXG-WP2dZ"
      },
      "source": [
        "Construct a method that performs dimensionality reduction on the matrix to produce k-dimensional embeddings. Use SVD to take the top k components and produce a new matrix of k-dimensional embeddings.\n",
        "\n",
        "Note: All of numpy, scipy, and scikit-learn (sklearn) provide some implementation of SVD, but only scipy and sklearn provide an implementation of Truncated SVD, and only sklearn provides an efficient randomized algorithm for calculating large-scale Truncated SVD. So please use sklearn.decomposition.TruncatedSVD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "5jfqvOUOP8R6"
      },
      "outputs": [],
      "source": [
        "def reduce_to_k_dim(M, k=2):\n",
        "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
        "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
        "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
        "    \n",
        "        Params:\n",
        "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\n",
        "            k (int): embedding size of each word after dimension reduction\n",
        "        Return:\n",
        "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
        "                    In terms of the SVD from math class, this actually returns U * S\n",
        "    \"\"\"    \n",
        "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
        "    M_reduced = None\n",
        "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
        "    \n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    svd = TruncatedSVD(n_components=k, n_iter=n_iters)\n",
        "    M_reduced = svd.fit_transform(M)\n",
        "    # ------------------\n",
        "\n",
        "    print(\"Done.\")\n",
        "    return M_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rGeaWNuRAnJ",
        "outputId": "cea7a2b5-0f69-486a-e7e9-e82bc80df9c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Truncated SVD over 10 words...\n",
            "Done.\n",
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness \n",
        "# In fact we only check that your M_reduced has the right dimensions.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and run student code\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "M_test, word2ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "M_test_reduced = reduce_to_k_dim(M_test, k=2)\n",
        "\n",
        "# Test proper dimensions\n",
        "assert (M_test_reduced.shape[0] == 10), \"M_reduced has {} rows; should have {}\".format(M_test_reduced.shape[0], 10)\n",
        "assert (M_test_reduced.shape[1] == 2), \"M_reduced has {} columns; should have {}\".format(M_test_reduced.shape[1], 2)\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iTgMaquRQKB"
      },
      "source": [
        "### Question 2.4: Implement plot_embeddings [code] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H629WACPRTg2"
      },
      "source": [
        "Here you will write a function to plot a set of 2D vectors in 2D space. For graphs, we will use Matplotlib (plt).\n",
        "\n",
        "For this example, you may find it useful to adapt this code. In the future, a good way to make a plot is to look at the Matplotlib gallery, find a plot that looks somewhat like what you want, and adapt the code they give."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "lMfaxKfERT1P"
      },
      "outputs": [],
      "source": [
        "def plot_embeddings(M_reduced, word2ind, words):\n",
        "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
        "        NOTE: do not plot all the words listed in M_reduced / word2ind.\n",
        "        Include a label next to each point.\n",
        "        \n",
        "        Params:\n",
        "            M_reduced (numpy matrix of shape (number of unique words in the corpus , 2)): matrix of 2-dimensioal word embeddings\n",
        "            word2ind (dict): dictionary that maps word to indices for matrix M\n",
        "            words (list of strings): words whose embeddings we want to visualize\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    x = M_reduced[:, 0]\n",
        "    y = M_reduced[:, 1]\n",
        "    plt.scatter(x, y)\n",
        "    for word in words:\n",
        "        idx = word2ind[word]\n",
        "        plt.annotate(word, (x[idx], y[idx]))\n",
        "    plt.show()\n",
        "    # ------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "bJ5sOXmXRYOa",
        "outputId": "45c0b612-93c7-4a3b-ce3b-7f241ae3bdff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Outputted Plot:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness.\n",
        "# The plot produced should look like the \"test solution plot\" depicted below. \n",
        "# ---------------------\n",
        "\n",
        "print (\"-\" * 80)\n",
        "print (\"Outputted Plot:\")\n",
        "\n",
        "M_reduced_plot_test = np.array([[1, 1], [-1, -1], [1, -1], [-1, 1], [0, 0]])\n",
        "word2ind_plot_test = {'test1': 0, 'test2': 1, 'test3': 2, 'test4': 3, 'test5': 4}\n",
        "words = ['test1', 'test2', 'test3', 'test4', 'test5']\n",
        "plot_embeddings(M_reduced_plot_test, word2ind_plot_test, words)\n",
        "\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRb0HnWqVCDL"
      },
      "source": [
        "### Question 2.5: Co-Occurrence Plot Analysis [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkfAWdLFVFx-"
      },
      "source": [
        "Now we will put together all the parts you have written! We will compute the co-occurrence matrix with fixed window of 4 (the default window size), over the Reuters \"crude\" (oil) corpus. Then we will use TruncatedSVD to compute 2-dimensional embeddings of each word. TruncatedSVD returns U*S, so we need to normalize the returned vectors, so that all the vectors will appear around the unit circle (therefore closeness is directional closeness). Note: The line of code below that does the normalizing uses the NumPy concept of broadcasting. If you don't know about broadcasting, check out Computation on Arrays: Broadcasting by Jake VanderPlas.\n",
        "\n",
        "Run the below cell to produce the plot. It'll probably take a few seconds to run. What clusters together in 2-dimensional embedding space? What doesn't cluster together that you might think should have? Note: \"bpd\" stands for \"barrels per day\" and is a commonly used abbreviation in crude oil topic articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "R3d0UK2iVFDM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "68326029-8949-433f-c828-6c7c38f6f7aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Truncated SVD over 8185 words...\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Run This Cell to Produce Your Plot\n",
        "# ------------------------------\n",
        "reuters_corpus = read_corpus()\n",
        "M_co_occurrence, word2ind_co_occurrence = compute_co_occurrence_matrix(reuters_corpus)\n",
        "M_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, k=2)\n",
        "\n",
        "# Rescale (normalize) the rows to make them each of unit-length\n",
        "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
        "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting\n",
        "\n",
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']\n",
        "\n",
        "plot_embeddings(M_normalized, word2ind_co_occurrence, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQg7l284V0oa"
      },
      "source": [
        "# Part 3. Prediction-based word vectors (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSNq-K6UzzDP"
      },
      "source": [
        "As discussed in class, more recently prediction-based word vectors have demonstrated better performance, such as word2vec and GloVe (which also utilizes the benefit of counts). If you're feeling adventurous, challenge yourself and try reading GloVe's original paper.\n",
        "\n",
        "Then run the following cells to load the GloVe vectors into memory. Note: If this is your first time to run these cells, i.e. download the embedding model, it will take a couple minutes to run. If you've run these cells before, rerunning them will load the model without redownloading it, which will take about 1 to 2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqq7A2IWz011"
      },
      "outputs": [],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(wv_from_bin.vocab.keys()))\n",
        "    return wv_from_bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYbJ59Jiz7OA"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------\n",
        "# Run Cell to Load Word Vectors\n",
        "# Note: This will take a couple minutes\n",
        "# -----------------------------------\n",
        "wv_from_bin = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edzctdyh0rDm"
      },
      "source": [
        "#### Note: If you are receiving a \"reset by peer\" error, rerun the cell to restart the download."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbGSRVPxjaT_"
      },
      "source": [
        "## Reducing dimensionality of Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXr1zGXSjddn"
      },
      "source": [
        "Let's directly compare the GloVe embeddings to those of the co-occurrence matrix. In order to avoid running out of memory, we will work with a sample of 10000 GloVe vectors instead. Run the following cells to:\n",
        "\n",
        "Put 10000 Glove vectors into a matrix M\n",
        "Run reduce_to_k_dim (your Truncated SVD function) to reduce the vectors from 200-dimensional to 2-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_xj1ApzjfOr"
      },
      "outputs": [],
      "source": [
        "def get_matrix_of_vectors(wv_from_bin, required_words=['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']):\n",
        "    \"\"\" Put the GloVe vectors into a matrix M.\n",
        "        Param:\n",
        "            wv_from_bin: KeyedVectors object; the 400000 GloVe vectors loaded from file\n",
        "        Return:\n",
        "            M: numpy matrix shape (num words, 200) containing the vectors\n",
        "            word2ind: dictionary mapping each word to its row number in M\n",
        "    \"\"\"\n",
        "    import random\n",
        "    words = list(wv_from_bin.vocab.keys())\n",
        "    print(\"Shuffling words ...\")\n",
        "    random.seed(224)\n",
        "    random.shuffle(words)\n",
        "    words = words[:10000]\n",
        "    print(\"Putting %i words into word2ind and matrix M...\" % len(words))\n",
        "    word2ind = {}\n",
        "    M = []\n",
        "    curInd = 0\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    for w in required_words:\n",
        "        if w in words:\n",
        "            continue\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    M = np.stack(M)\n",
        "    print(\"Done.\")\n",
        "    return M, word2ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHVTZLiBjhN5"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------\n",
        "# Run Cell to Reduce 200-Dimensional Word Embeddings to k Dimensions\n",
        "# Note: This should be quick to run\n",
        "# -----------------------------------------------------------------\n",
        "M, word2ind = get_matrix_of_vectors(wv_from_bin)\n",
        "M_reduced = reduce_to_k_dim(M, k=2)\n",
        "\n",
        "# Rescale (normalize) the rows to make them each of unit-length\n",
        "M_lengths = np.linalg.norm(M_reduced, axis=1)\n",
        "M_reduced_normalized = M_reduced / M_lengths[:, np.newaxis] # broadcasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdoZKWxijmHk"
      },
      "source": [
        "### Question 3.1: GloVe Plot Analysis [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyDIugqjjo3R"
      },
      "source": [
        "Run the cell below to plot the 2D GloVe embeddings for ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq'].\n",
        "\n",
        "What clusters together in 2-dimensional embedding space? What doesn't cluster together that you think should have? How is the plot different from the one generated earlier from the co-occurrence matrix? What is a possible cause for the difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK438bCgjm98"
      },
      "outputs": [],
      "source": [
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']\n",
        "plot_embeddings(M_reduced_normalized, word2ind, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39cCl0DQjuJN"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toHZ2o-Ljwwm"
      },
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBZbtLYzj1R_"
      },
      "source": [
        "Now that we have word vectors, we need a way to quantify the similarity between individual words, according to these vectors. One such metric is cosine-similarity. We will be using this to find words that are \"close\" and \"far\" from one another.\n",
        "\n",
        "We can think of n-dimensional vectors as points in n-dimensional space. If we take this perspective L1 and L2 Distances help quantify the amount of space \"we must travel\" to get between these two points. Another approach is to examine the angle between two vectors. From trigonometry we know that:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLE4DOTNj69N"
      },
      "source": [
        "### Question 3.2: Words with Multiple Meanings (1.5 points) [code + written]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0LT0bH4kolN"
      },
      "source": [
        "Polysemes and homonyms are words that have more than one meaning (see this wiki page to learn more about the difference between polysemes and homonyms ). Find a word with at least two different meanings such that the top-10 most similar words (according to cosine similarity) contain related words from both meanings. For example, \"leaves\" has both \"go_away\" and \"a_structure_of_a_plant\" meaning in the top 10, and \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\". You will probably need to try several polysemous or homonymic words before you find one.\n",
        "\n",
        "Please state the word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous or homonymic words you tried didn't work (i.e. the top-10 most similar words only contain one of the meanings of the words)?\n",
        "\n",
        "Note: You should use the wv_from_bin.most_similar(word) function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance, please check the GenSim documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgoE4V3rj76o"
      },
      "outputs": [],
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z90e-p_jktYD"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBt1A0Y8kyqx"
      },
      "source": [
        "### Question 3.3: Synonyms & Antonyms (2 points) [code + written]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2gWr_Cvk3Tu"
      },
      "source": [
        "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
        "\n",
        "Find three words  (w1,w2,w3)  where  w1  and  w2  are synonyms and  w1  and  w3  are antonyms, but Cosine Distance  (w1,w3)<  Cosine Distance  (w1,w2) .\n",
        "\n",
        "As an example,  w1 =\"happy\" is closer to  w3 =\"sad\" than to  w2 =\"cheerful\". Please find a different example that satisfies the above. Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the wv_from_bin.distance(w1, w2) function here in order to compute the cosine distance between two words. Please see the GenSim documentation for further assistance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRwHp4noktJF"
      },
      "outputs": [],
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AD2asvrk7Y7"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0uNtlXZlAdy"
      },
      "source": [
        "### Question 3.4: Analogies with Word Vectors [written] (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqKCiSDhlEBf"
      },
      "source": [
        "Word vectors have been shown to sometimes exhibit the ability to solve analogies.\n",
        "\n",
        "As an example, for the analogy \"man : king :: woman : x\" (read: man is to king as woman is to x), what is x?\n",
        "\n",
        "In the cell below, we show you how to use word vectors to find x using the most_similar function from the GenSim documentation. The function finds words that are most similar to the words in the positive list and most dissimilar from the words in the negative list (while omitting the input words, which are often the most similar; see this paper). The answer to the analogy will have the highest cosine similarity (largest returned numerical value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHlsY4kolA6f"
      },
      "outputs": [],
      "source": [
        "# Run this cell to answer the analogy -- man : king :: woman : x\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'king'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv1gqPl5nQ4H"
      },
      "source": [
        "Let  m ,  k ,  w , and  x  denote the word vectors for man, king, woman, and the answer, respectively. Using only vectors  m ,  k ,  w , and the vector arithmetic operators  +  and    in your answer, what is the expression in which we are maximizing cosine similarity with  x ?\n",
        "\n",
        "Hint: Recall that word vectors are simply multi-dimensional vectors that represent a word. It might help to draw out a 2D example using arbitrary locations of each vector. Where would man and woman lie in the coordinate plane relative to king and the answer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zvj-YaOnTAY"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLyzhOOfnuql"
      },
      "source": [
        "### Question 3.5: Finding Analogies [code + written] (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5xFEsKVnzT0"
      },
      "source": [
        "Find an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy is complicated, explain why the analogy holds in one or two sentences.\n",
        "\n",
        "Note: You may have to try many analogies to find one that works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkrWfGGznRMH"
      },
      "outputs": [],
      "source": [
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NygUqza7n8mn"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMbSmc52n-Ni"
      },
      "source": [
        "### Question 3.6: Incorrect Analogy [code + written] (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKhHs5uooAaD"
      },
      "source": [
        "Find an example of analogy that does not hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the (incorrect) value of b according to the word vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayAfn_MPnzrF"
      },
      "outputs": [],
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsqq_EaXoDYl"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sBlWGAmoGCy"
      },
      "source": [
        "### Question 3.7: Guided Analysis of Bias in Word Vectors [written] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_rVPQteoINq"
      },
      "source": [
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n",
        "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"worker\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"worker\" and most dissimilar to \"woman\". Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDlvsts2oBsp"
      },
      "outputs": [],
      "source": [
        "# Run this cell\n",
        "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
        "# most dissimilar from.\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'worker'], negative=['man']))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'worker'], negative=['woman']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BtOxfydoLZb"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMsfYJyxoNWT"
      },
      "source": [
        "###  Question 3.8: Independent Analysis of Bias in Word Vectors [code + written] (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h42ajQDcodeV"
      },
      "source": [
        "Use the most_similar function to find another case where some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9d5cbx3oJsf"
      },
      "outputs": [],
      "source": [
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDBxYCeEolk5"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNaetQ4donRi"
      },
      "source": [
        "### Question 3.9: Thinking About Bias [written] (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mktU3tdqtzp"
      },
      "source": [
        "Give one explanation of how bias gets into the word vectors. What is an experiment that you could do to test for or to measure this source of bias?\n",
        "\n",
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrXQ2d7OsmLl"
      },
      "source": [
        "# Part 4. Prediction-based sentence vectors (13 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOP9j0mV2rvU"
      },
      "source": [
        "Sentence embeddings are a more powerful representation than word embeddings. They allow you to have out-of-the-box sentence representation of sequences of tokens which is closer to what you would have in reality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ3pVRQ8wE4P"
      },
      "source": [
        "### Question 4.1: How would you represent a sentence with Glove? What are the limits of your proposed implementation? [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1w30iYj0YJn"
      },
      "source": [
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2n3vp8g3AEe"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9F_tbMr3CZT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBTJnO606Tpc"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnidR9Gg2697"
      },
      "outputs": [],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load SentenceBERT Vectors\n",
        "        Return:\n",
        "            embedder: sentence embedder \n",
        "    \"\"\"\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    \n",
        "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    return embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qzL9oBS4Zoc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "embedder = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKkFzRHe6bEm"
      },
      "source": [
        "Inspired by the above, choose the appropriate way to plot the below clusters. Do they make sense to you? What would you improve to get a meaningful plot?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-o3T8Wz-Feb"
      },
      "source": [
        "### Question 4.2. Evaluate clustering quality of SentenceBERT. What makes it good at clustering sentences? Which method of the two below would you go for? [written] (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqHOmWtqyNo3"
      },
      "outputs": [],
      "source": [
        "# Corpus with example sentences\n",
        "corpus = ['A man is eating food.',\n",
        "          'A man is eating a piece of bread.',\n",
        "          'A man is eating pasta.',\n",
        "          'The girl is carrying a baby.',\n",
        "          'The baby is carried by the woman',\n",
        "          'A man is riding a horse.',\n",
        "          'A man is riding a white horse on an enclosed ground.',\n",
        "          'A monkey is playing drums.',\n",
        "          'Someone in a gorilla costume is playing a set of drums.',\n",
        "          'A cheetah is running behind its prey.',\n",
        "          'A cheetah chases prey on across a field.'\n",
        "          ]\n",
        "corpus_embeddings = embedder.encode(corpus)\n",
        "\n",
        "# Perform kmean clustering\n",
        "num_clusters = 5\n",
        "clustering_model = KMeans(n_clusters=num_clusters)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = [[] for i in range(num_clusters)]\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "\n",
        "for i, cluster in enumerate(clustered_sentences):\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4wF5uTy50Nt"
      },
      "outputs": [],
      "source": [
        "# Corpus with example sentences\n",
        "corpus = ['A man is eating food.',\n",
        "          'A man is eating a piece of bread.',\n",
        "          'A man is eating pasta.',\n",
        "          'The girl is carrying a baby.',\n",
        "          'The baby is carried by the woman',\n",
        "          'A man is riding a horse.',\n",
        "          'A man is riding a white horse on an enclosed ground.',\n",
        "          'A monkey is playing drums.',\n",
        "          'Someone in a gorilla costume is playing a set of drums.',\n",
        "          'A cheetah is running behind its prey.',\n",
        "          'A cheetah chases prey on across a field.'\n",
        "          ]\n",
        "corpus_embeddings = embedder.encode(corpus)\n",
        "\n",
        "# Normalize the embeddings to unit length\n",
        "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Perform kmean clustering\n",
        "clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5) #, affinity='cosine', linkage='average', distance_threshold=0.4)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    if cluster_id not in clustered_sentences:\n",
        "        clustered_sentences[cluster_id] = []\n",
        "\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "\n",
        "for i, cluster in clustered_sentences.items():\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44mDLKmVyOUu"
      },
      "source": [
        "### Question 4.3: SentenceBERT Plot Analysis [written] (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_ykELvWt7ZX"
      },
      "source": [
        "Plot the above corpus with your favorite method in a 2-dimensional space. Comment on the output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXqSyMyy-bm8"
      },
      "source": [
        "### Question 4.4: Independent Analysis of Bias in Word Vectors [code + written] (4 points) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZynvZnq-nqk"
      },
      "source": [
        "Select a corpus of interest, or examples of interest and shed light on one source of bias from SentenceBERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQyOEVUWvEmI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rKtcL0CYn2n_",
        "C2IElpuJ76o0",
        "BHC9ShM-IX7E",
        "KK7YbNCyJaqk",
        "Wao4NivXGPIM",
        "A1wbWizrJ5CE",
        "x9hzsg0CUQZ_",
        "mhS_Y6qgqW4Z",
        "wRb0HnWqVCDL",
        "hQg7l284V0oa",
        "qbGSRVPxjaT_",
        "cdoZKWxijmHk",
        "toHZ2o-Ljwwm",
        "QMbSmc52n-Ni",
        "hrXQ2d7OsmLl"
      ],
      "name": "Lab1_EPITA.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}